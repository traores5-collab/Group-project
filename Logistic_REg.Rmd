```{r}
data <- read.csv("diabetes2.csv") 
```

```{r}
data
```

```{r}
data_matrix <- as.matrix(data)
```

```{r}
data_matrix
```

```{r}
X <- data_matrix[, 1:8]  

```

```{r}
X
```
```{r}
# Selects the target column: Outcome
y <- data_matrix[, 9, drop = FALSE] 

```

```{r}
y
```

```{r}

# Columns with known 0s representing missing data:
critical_cols <- c(2, 3, 4, 5, 6) # Glucose, BloodPressure, SkinThickness, Insulin, BMI

# Replace 0s with NA in X
for(i in critical_cols) {
  X[X[, i] == 0, i] <- NA
}

# Fill the NAs with the MEDIAN (more robust than mean) of the column
for(i in critical_cols) {
  median_val <- median(X[, i], na.rm = TRUE)
  X[is.na(X[, i]), i] <- median_val
}
```

```{r}
# 1. Normalize the Features (X is now clean)
X_scaled <- scale(X)

# 2. Add the Intercept Column (X0 = 1)
m <- length(y)
X_extend <- cbind(rep(1, m), X_scaled)
```

```{r}
# 1. Sigmoid Function (Hypothesis)
sigmoid <- function(z) {
  return(1 / (1 + exp(-z)))
}

# 2. Numerically Stable Cost Function (with clipping)
costFunction <- function(theta, X, y) {
  m <- length(y)
  h <- sigmoid(X %*% theta)
  
  # CRITICAL FIX: Clipping h to prevent log(0) and log(1) errors
  epsilon <- 1e-15
  h[h > (1 - epsilon)] <- 1 - epsilon
  h[h < epsilon] <- epsilon
  
  # Cost calculation (Negative Log-Likelihood)
  j <- (-1 / m) * (t(log(h)) %*% y + t(log(1 - h)) %*% (1 - y))
  
  return(as.numeric(j[1, 1]))
}

# 3. Vectorized Gradient Descent Function
gradient_descent <- function(X, y, alpha = 0.1, num_iters = 1500) {
  m <- length(y)
  theta <- matrix(0, nrow = ncol(X), ncol = 1)
  j_history <- numeric(num_iters)
  
  for (i in 1:num_iters) {
    h <- sigmoid(X %*% theta)
    
    # Calculate Gradient (Vectorized formula)
    grad <- (1 / m) * t(X) %*% (h - y)
    
    # Update Theta
    theta <- theta - alpha * grad
    
    # Store Cost
    j_history[i] <- costFunction(theta, X, y)
  }
  
  return(list(theta = theta, j_history = j_history))
}
```

```{r}
# Running the training: Ensure X_extend and y are already defined from previous steps
# (cleaned, scaled, and intercept added)
result <- gradient_descent(X = X_extend, y = y, alpha = 0.1, num_iters = 1500)

theta_final <- result$theta
j_history <- result$j_history

cat("Final Model Parameters (Theta):\n")
print(theta_final)
```

```{r}
# Plot the cost history to check for convergence
plot(j_history, type = "l", 
     xlab = "Number of Iterations", 
     ylab = "Cost J (Negative Log-Likelihood)",
     main = "Convergence of Logistic Regression")
```

```{r}
# --- 1. Prediction Function ---
# Predicts the outcome (0 or 1) based on the sigmoid output (h)
predict_outcome <- function(theta, X) {
  # Calculate h = sigmoid(X * theta)
  h <- sigmoid(X %*% theta)
  
  # If h >= 0.5, predict 1 (Diabetes); otherwise, predict 0 (No Diabetes)
  predictions <- ifelse(h >= 0.5, 1, 0)
  
  return(predictions)
}

# --- 2. Calculate Accuracy ---
# Generate predictions using the final parameters
predictions <- predict_outcome(theta_final, X_extend)

# Compare predictions to the true target values (y)
accuracy <- mean(predictions == y)

cat("Model Training Accuracy:\n", accuracy * 100, "%\n")
```
