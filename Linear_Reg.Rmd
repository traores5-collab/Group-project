```{r}
data <- read.csv("Student_Performance.csv") 

```

```{r}
head(data)
```

```{r}
# R Code to achieve 0/1 encoding:
data$Extracurricular.Activities <- ifelse(data$Extracurricular.Activities == "Yes", 1, 0)

# Display the data frame (equivalent to printing 'data' in Python)
print(data)
```


```{r}
Data<- na.omit(data)
```

```{r}
colSums(is.na(Data))
```

```{r}
Data <- as.matrix(Data)
```

```{r}
X <- Data[, c(1, 2,3,4,5)]  # Select columns 1 and 3
y <- Data[, 3, drop = FALSE]  # Select column 2 and keep it as a 2D matrix
```

```{r}
X <- scale(X) 
```

```{r}
m <- length(y)
x_extend <- cbind(rep(1, m), X)
x_extend
```

```{r}
# 1. Determine the number of features + intercept (6 total)
num_params <- ncol(x_extend)

# 2. Initialize theta to the correct size (6 rows, 1 column)
initial_theta <- matrix(0, nrow = num_params, ncol = 1)

# 3. Call the function with the correctly sized theta
cost <- costfunction(x_extend, y, initial_theta)


# Call the cost function, passing the initialized theta
cost <- costfunction(x_extend, y, initial_theta)

cat("Initial Cost (MSE/2):\n", cost, "\n")
```


```{r}
# 1. Ensure theta is initialized correctly (6 parameters: 1 intercept + 5 features)
num_params <- ncol(x_extend)
initial_theta <- matrix(0, nrow = num_params, ncol = 1)

# 2. Execute the generalized Gradient Descent
# Using alpha=0.1 and num_iters=1500 for good convergence
result <- gradient_descent(X = x_extend, y = y, theta = initial_theta, alpha = 0.1, num_iters = 1500)
theta_final <- result$theta
j_history <- result$j_history

cat("Final Model Parameters (Theta):\n")
print(theta_final)
```




